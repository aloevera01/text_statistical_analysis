{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fancy-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scipy\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "functioning-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header_footer(text, name_book):\n",
    "    header = \"*** START OF THE PROJECT GUTENBERG EBOOK \" + name_book.upper() + \" ***\"\n",
    "    footer = \"*** END OF THE PROJECT GUTENBERG EBOOK \" + name_book.upper() + \" ***\"\n",
    "    no_header_text = text[(text.find(header) + len(header)):]\n",
    "    no_footer_text = no_header_text[:no_header_text.find(footer)]\n",
    "    \n",
    "    return no_footer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "appropriate-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    temp_tokenizedtext = nltk.word_tokenize(text)    \n",
    "    mycrawled_nltktext = nltk.Text(temp_tokenizedtext)    \n",
    "    \n",
    "    return mycrawled_nltktext\n",
    "\n",
    "\n",
    "def lower_case_text(text):\n",
    "    mycrawled_lowercasetext = [] \n",
    "\n",
    "    for k in range(len(text)):        \n",
    "        lowercaseword = text[k].lower()        \n",
    "        mycrawled_lowercasetext.append(lowercaseword)    \n",
    "        \n",
    "    return mycrawled_lowercasetext\n",
    "\n",
    "def tagtowordnet(postag):   \n",
    "    wordnettag = -1   \n",
    "    if postag[0] == 'N':        \n",
    "        wordnettag = 'n'   \n",
    "    elif postag[0] == 'V':        \n",
    "        wordnettag = 'v'   \n",
    "    elif postag[0] == 'J':        \n",
    "        wordnettag = 'a'    \n",
    "    elif postag[0] == 'R':        \n",
    "        wordnettag = 'r'    \n",
    "    return(wordnettag)\n",
    "\n",
    "def lemmatizetext(nltktexttolemmatize):    \n",
    "    # Tag the text with POS tags    \n",
    "    taggedtext = nltk.pos_tag(nltktexttolemmatize)   \n",
    "    # Lemmatize each word text    \n",
    "    lemmatizedtext = []    \n",
    "    for l in range(len(taggedtext)):       \n",
    "        # Lemmatize a word using the WordNet converted POS tag       \n",
    "        wordtolemmatize = taggedtext[l][0]        \n",
    "        wordnettag = tagtowordnet(taggedtext[l][1])        \n",
    "        if wordnettag != -1:            \n",
    "            lemmatizedword = lemmatizer.lemmatize(wordtolemmatize,wordnettag)        \n",
    "        else:            \n",
    "            lemmatizedword=wordtolemmatize       \n",
    "            # Store the lemmatized word        \n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "        \n",
    "    return(lemmatizedtext) \n",
    "\n",
    "\n",
    "def make_vocabulary(text):\n",
    "    myvocabulary = [] \n",
    "    myindices_in_vocabulary = []\n",
    "    # Find the vocabulary of each document    \n",
    "    # Get unique words and where they occur      \n",
    "    uniqueresults = np.unique(text,return_inverse=True)   \n",
    "     # Store the vocabulary and indices\n",
    "    myvocabulary = uniqueresults[0]    \n",
    "    myindices_in_vocabulary = uniqueresults[1]    \n",
    "    \n",
    "    return myvocabulary, myindices_in_vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-relay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "sophisticated-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_text(mycrawled_lemmatizedtexts, unifiedvocabulary, myindices_in_unifiedvocabulary,  highest_totaloccurrences_indices, occur_counts):\n",
    "    nltkstopwords=nltk.corpus.stopwords.words('english') \n",
    "    pruningdecisions = np.zeros((len(unifiedvocabulary),1)) \n",
    "    for k in range(len(unifiedvocabulary)):    \n",
    "        # Rule 1: check the nltk stop word list    \n",
    "        if (unifiedvocabulary[k] in nltkstopwords):        \n",
    "            pruningdecisions[k] = 1    \n",
    "            \n",
    "        # Rule 2: if the word is in the top 1% of frequent words    \n",
    "        if (k in highest_totaloccurrences_indices[\\\n",
    "                                                  0:int(np.floor(len(unifiedvocabulary)*0.01))]):        \n",
    "            pruningdecisions[k] = 1    \n",
    "        # Rule 3: if the word is too short    \n",
    "        if len(unifiedvocabulary[k]) < 2:        \n",
    "            pruningdecisions[k] = 1    \n",
    "            \n",
    "        # Rule 4: if the word is too long    \n",
    "        if len(unifiedvocabulary[k])>20:        \n",
    "            pruningdecisions[k] = 1    \n",
    "            \n",
    "        # Rule 5: if the word appears less than 4 times\n",
    "        if occur_counts[k] < 4:\n",
    "            pruningdecisions[k] = 1  \n",
    "         \n",
    "    oldtopruned = [] \n",
    "    tempind = -1 \n",
    "    for k in range(len(unifiedvocabulary)):    \n",
    "        if pruningdecisions[k] == 0:        \n",
    "            tempind = tempind + 1        \n",
    "            oldtopruned.append(tempind)    \n",
    "        else:        \n",
    "            oldtopruned.append(-1) \n",
    "    #%% Create pruned texts \n",
    "    mycrawled_prunedtexts = [] \n",
    "    myindices_in_prunedvocabulary = [] \n",
    "    for k in range(len(mycrawled_lemmatizedtexts)):       \n",
    "        temp_newindices = []    \n",
    "        temp_newdoc = []    \n",
    "        for l in range(len(mycrawled_lemmatizedtexts[k])):        \n",
    "            temp_oldindex = myindices_in_unifiedvocabulary[k][l]                    \n",
    "            temp_newindex = oldtopruned[temp_oldindex]        \n",
    "            if temp_newindex != -1:            \n",
    "                temp_newindices.append(temp_newindex)            \n",
    "                temp_newdoc.append(unifiedvocabulary[temp_oldindex])    \n",
    "        mycrawled_prunedtexts.append(temp_newdoc)    \n",
    "        myindices_in_prunedvocabulary.append(temp_newindices)\n",
    "    \n",
    "    remainingindices = np.squeeze(np.where(pruningdecisions == 0)[0]) \n",
    "    remainingvocabulary = unifiedvocabulary[remainingindices] \n",
    "    return mycrawled_prunedtexts, myindices_in_prunedvocabulary, remainingvocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "applicable-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "wizard_oz_html = requests.get('https://www.gutenberg.org/files/55/55-0.txt')\n",
    "wizard_oz_text = str(bs4.BeautifulSoup(wizard_oz_html.content,'html.parser'))\n",
    "wizard_oz_text = remove_header_footer(wizard_oz_text, 'THE WONDERFUL WIZARD OF OZ')\n",
    "\n",
    "wizard_oz_paragraphs = re.split('\\r\\n\\r\\n[\\r\\n]*', wizard_oz_text) \n",
    "\n",
    "wizard_oz_texts = []\n",
    "for par in wizard_oz_paragraphs:\n",
    "    par = \" \".join(par.split())\n",
    "    wizard_oz_texts.append(par)\n",
    "    \n",
    "wizard_oz_tokenized = []\n",
    "for par in wizard_oz_texts:\n",
    "    par = tokenize_text(par)\n",
    "    wizard_oz_tokenized.append(par)\n",
    "    \n",
    "\n",
    "wizard_oz_texts_lower = []\n",
    "for par in wizard_oz_tokenized:\n",
    "    par = lower_case_text(par)\n",
    "    wizard_oz_texts_lower.append(par)\n",
    "    \n",
    "\n",
    "wizard_oz_texts_lemmatized = []\n",
    "for par in wizard_oz_texts_lower:\n",
    "    par = lemmatizetext(par)\n",
    "    par = nltk.Text(par)\n",
    "    wizard_oz_texts_lemmatized.append(par)\n",
    "    \n",
    "vocabularies = [] \n",
    "indices_in_vocabularies = []\n",
    "# Find the vocabulary of each document \n",
    "for par in wizard_oz_texts_lemmatized:   \n",
    "    # Get unique words and where they occur       \n",
    "    uniqueresults = np.unique(par,return_inverse=True)    \n",
    "    uniquewords = uniqueresults[0]    \n",
    "    wordindices = uniqueresults[1]    \n",
    "    # Store the vocabulary and indices of document words in it    \n",
    "    vocabularies.append(uniquewords)    \n",
    "    indices_in_vocabularies.append(wordindices) \n",
    "    \n",
    "tempvocabulary = []  \n",
    "for par in wizard_oz_texts_lemmatized:   \n",
    "    tempvocabulary.extend(par) \n",
    "    # Find the unique elements among all vocabularies \n",
    "uniqueresults = np.unique(tempvocabulary,return_inverse=True) \n",
    "unifiedvocabulary = uniqueresults[0] \n",
    "wordindices = uniqueresults[1] \n",
    "# Translate previous indices to the unified vocabulary. \n",
    "# Must keep track where each vocabulary started in \n",
    "# the concatenated one. \n",
    "vocabularystart = 0 \n",
    "indices_in_unifiedvocabulary = [] \n",
    "for k in range(len(wizard_oz_texts_lemmatized)):    \n",
    "    # In order to shift word indices, we must temporarily    \n",
    "    # change their data type to a Numpy array    \n",
    "    tempindices = np.array(indices_in_vocabularies[k])    \n",
    "    tempindices = tempindices + vocabularystart    \n",
    "    tempindices = wordindices[tempindices]    \n",
    "    indices_in_unifiedvocabulary.append(tempindices)    \n",
    "    vocabularystart = vocabularystart + len(vocabularies[k])\n",
    "    \n",
    "unifiedvocabulary_totaloccurrencecounts = np.zeros((len(unifiedvocabulary),1)) \n",
    "\n",
    "for k in range(len(wizard_oz_texts_lemmatized)): \n",
    "    occurrencecounts = np.zeros((len(unifiedvocabulary),1))    \n",
    "    for l in range(len(indices_in_unifiedvocabulary[k])):        \n",
    "        occurrencecounts[indices_in_unifiedvocabulary[k][l]] = \\\n",
    "        occurrencecounts[indices_in_unifiedvocabulary[k][l]] + 1    \n",
    "    unifiedvocabulary_totaloccurrencecounts = \\\n",
    "    unifiedvocabulary_totaloccurrencecounts + occurrencecounts  \n",
    "\n",
    "highest_totaloccurrences_indices = np.argsort(\\\n",
    "                                               -1*unifiedvocabulary_totaloccurrencecounts,axis=0) \n",
    "\n",
    "wizard_oz_pruned_texts, vocabulary_pruned_indices, vocabulary_pruned = prune_text(wizard_oz_texts_lemmatized, unifiedvocabulary, \n",
    "                                                                                  indices_in_unifiedvocabulary, \n",
    "                                                                                  unifiedvocabulary_totaloccurrencecounts,\n",
    "                                                                                  highest_totaloccurrences_indices)\n",
    "                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ultimate-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = len(wizard_oz_pruned_texts) \n",
    "n_vocab = len(vocabulary_pruned) \n",
    "# Matrix of term frequencies \n",
    "tfmatrix = scipy.sparse.lil_matrix((n_docs,n_vocab)) \n",
    "# Row vector of document frequencies \n",
    "dfvector = scipy.sparse.lil_matrix((1,n_vocab)) \n",
    "# Loop over documents \n",
    "for k in range(n_docs):    \n",
    "    # Row vector of which words occurred in this document    \n",
    "    temp_dfvector = scipy.sparse.lil_matrix((1,n_vocab))   \n",
    "    temp_total_terms = len(np.unique(wizard_oz_pruned_texts[k]))\n",
    "    # Loop over words    \n",
    "    for l in range(len(wizard_oz_pruned_texts[k])):        \n",
    "    # Add current word to term-frequency count and document-count        \n",
    "        currentword = vocabulary_pruned_indices[k][l]        \n",
    "        tfmatrix[k,currentword] = tfmatrix[k,currentword] + 1        \n",
    "        temp_dfvector[0,currentword] = 1  \n",
    "    # Length normalization frequence of raw counts\n",
    "    if temp_total_terms == 0:\n",
    "        temp_total_terms = 1\n",
    "    tfmatrix[k,:] /= temp_total_terms\n",
    "    # Add which words occurred in this document to overall document counts\n",
    "    dfvector = dfvector + temp_dfvector\n",
    "    # Use the count statistics to compute the tf-idf matrix \n",
    "tfidfmatrix = scipy.sparse.lil_matrix((n_docs,n_vocab)) \n",
    "# Let's use length-normalized frequency term count, and smoothed logarithmic idf \n",
    "idfvector = 1 + np.log((1 / (np.array(dfvector.todense())[0] + 1)) * n_docs)\n",
    "for k in range(n_docs):    \n",
    "    # Combine the tf and idf terms    \n",
    "    tfidfmatrix[k,:] = tfmatrix[k,:] * idfvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "rural-separate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \\\\\n",
      "move mouth mouse mourning mourn motionless mother mess moth \\\\\n",
      "1 \\\\\n",
      "move mouth mouse mourning mourn motionless mother mess moth \\\\\n",
      "2 \\\\\n",
      "move mouth mouse mourning mourn motionless mother mess moth \\\\\n",
      "3 \\\\\n",
      "mouth mouse mourning mourn motionless moth moss mortal morning—all \\\\\n",
      "4 \\\\\n",
      "move mouth mouse mourning mourn motionless mother mess moth \\\\\n",
      "5 \\\\\n",
      "reply replace platter scarcely scar rock rocky mother mess \\\\\n",
      "6 \\\\\n",
      "move mouth mouse mourning mourn motionless mother mess moth \\\\\n",
      "7 \\\\\n",
      "move mouth mouse mourning mourn motionless mother mess moth \\\\\n",
      "8 \\\\\n",
      "move mouth mouse mourning mourn motionless mother mess moth \\\\\n",
      "9 \\\\\n",
      "move mouth mouse mourning mourn motionless mother mess moth \\\\\n"
     ]
    }
   ],
   "source": [
    "dimensiontotals = np.squeeze(np.array(\\\n",
    "                                          np.sum(tfidfmatrix,axis=0))) \n",
    "highesttotals = np.argsort(-1 * dimensiontotals) \n",
    "Xsmall = tfidfmatrix[:,highesttotals[0:500]] \n",
    "Xsmall = Xsmall.todense() \n",
    "# Normalize the documents to unit vector norm \n",
    "tempnorms = np.squeeze(np.array(np.sum(np.multiply(Xsmall,Xsmall),axis=1))) \n",
    "# If any documents have zero norm, avoid dividing them by zero \n",
    "tempnorms[tempnorms == 0] = 1 \n",
    "Xsmall = scipy.sparse.diags(tempnorms ** -0.5).dot(Xsmall) \n",
    "\n",
    "import sklearn    \n",
    "import sklearn.mixture \n",
    "# Create the mixture model object, and \n",
    "# choose the number of components and EM iterations \n",
    "mixturemodel = sklearn.mixture.GaussianMixture(n_components=10, \\\n",
    "                                             covariance_type='diag',max_iter=100,init_params='random') \n",
    "fittedmixture = mixturemodel.fit(Xsmall) \n",
    "sklearn_mixturemodel_means = fittedmixture.means_ \n",
    "sklearn_mixturemodel_weights = fittedmixture.weights_ \n",
    "sklearn_mixturemodel_covariances = fittedmixture.covariances_\n",
    "\n",
    "# Find top 10 words with highest mean feature value for each cluster \n",
    "for k in range(10):    \n",
    "    print(k, '\\\\\\\\')    \n",
    "    highest_dimensionweight_indices = np.argsort( \\\n",
    "                                                  -np.squeeze(sklearn_mixturemodel_means[k,:]),axis=0)    \n",
    "    highest_dimensionweight_indices = highesttotals[highest_dimensionweight_indices]    \n",
    "    print(' '.join(vocabulary_pruned[highest_dimensionweight_indices[1:10]]), '\\\\\\\\') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "elementary-pathology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09980762, 0.09985188, 0.10007271, 0.10031327, 0.09932124,\n",
       "       0.10004263, 0.09994624, 0.10134318, 0.09760854, 0.10169269])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "formal-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.matlib import repmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "interracial-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidfmatrix \n",
    "# Normalize the documents to unit vector norm \n",
    "tempnorms = np.squeeze(np.array(np.sum(X.multiply(X),axis=1))) \n",
    "# If any documents have zero norm, avoid dividing them by zero \n",
    "tempnorms[tempnorms == 0] = 1 \n",
    "X = scipy.sparse.diags(tempnorms ** -0.5).dot(X) \n",
    "n_data = np.shape(X)[0] \n",
    "n_dimensions = np.shape(X)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "frank-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mixturemodel(X,n_components):    \n",
    "    # Create lists of sparse matrices to hold the parameters    \n",
    "    n_dimensions = np.shape(X)[1]    \n",
    "    mixturemodel_means = scipy.sparse.lil_matrix((n_components,n_dimensions))    \n",
    "    mixturemodel_weights = np.zeros((n_components))    \n",
    "    mixturemodel_covariances = []    \n",
    "    mixturemodel_inversecovariances = []    \n",
    "    for k in range(n_components):        \n",
    "        tempcovariance = scipy.sparse.lil_matrix((n_dimensions,n_dimensions))            \n",
    "        mixturemodel_covariances.append(tempcovariance)        \n",
    "        tempinvcovariance = scipy.sparse.lil_matrix((n_dimensions,n_dimensions))           \n",
    "        mixturemodel_inversecovariances.append(tempinvcovariance)    \n",
    "    # Initialize the parameters    \n",
    "    for k in range(n_components):        \n",
    "        mixturemodel_weights[k] = 1/n_components        \n",
    "        # Pick a random data point as the initial mean        \n",
    "        tempindex=scipy.stats.randint.rvs(low=0,high=n_components)        \n",
    "        mixturemodel_means[k] = X[tempindex,:].toarray()          \n",
    "        # Initialize the covariance matrix to be spherical        \n",
    "        for l in range(n_dimensions):           \n",
    "            mixturemodel_covariances[k][l,l] = 1            \n",
    "            mixturemodel_inversecovariances[k][l,l] = 1    \n",
    "    return(mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "                                                                   mixturemodel_inversecovariances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "closed-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_estep(X,mixturemodel_means,mixturemodel_covariances, \\\n",
    "              mixturemodel_inversecovariances,mixturemodel_weights):    \n",
    "    # For each component, compute terms that do not involve data    \n",
    "    meanterms = np.zeros((n_components))    \n",
    "    logdeterminants = np.zeros((n_components))    \n",
    "    logconstantterms = np.zeros((n_components))    \n",
    "    for k in range(n_components):        \n",
    "        # Compute mu_k*inv(Sigma_k)*mu_k        \n",
    "        meanterms[k] = (mixturemodel_means[k,:] * \\\n",
    "                      mixturemodel_inversecovariances[k] * mixturemodel_means[k,:].T)[0,0]       \n",
    "        # Compute determinant of Sigma_k. For a diagonal matrix         \n",
    "        # this is just the product of the main diagonal        \n",
    "        logdeterminants[k] = np.sum(np.log(mixturemodel_covariances[k].diagonal(0)))        \n",
    "        # Compute constant term beta_k * 1/(|Sigma_k|^1/2)        \n",
    "        # Omit the (2pi)^d/2 as it cancels out        \n",
    "        logconstantterms[k] = np.log(mixturemodel_weights[k]) - 0.5 * logdeterminants[k]    \n",
    "    print('E-step part2 ')    \n",
    "    # Compute terms that involve distances of data from components    \n",
    "    xnorms = np.zeros((n_data,n_components))    \n",
    "    xtimesmu = np.zeros((n_data,n_components))    \n",
    "    for k in range(n_components):        \n",
    "        print(k)        \n",
    "        xnorms[:,k] = (X * mixturemodel_inversecovariances[k] * X.T).diagonal(0)        \n",
    "        xtimesmu[:,k] = np.squeeze((X * mixturemodel_inversecovariances[k] * \\\n",
    "                                     mixturemodel_means[k,:].T).toarray())    \n",
    "    xdists = xnorms + repmat(meanterms,n_data,1) - 2 * xtimesmu            \n",
    "    # Substract maximal term before exponent (cancels out) to maintain computational precision    \n",
    "    numeratorterms = logconstantterms - xdists / 2    \n",
    "    numeratorterms -= repmat(np.max(numeratorterms,axis=1),n_components,1).T    \n",
    "    numeratorterms = np.exp(numeratorterms)    \n",
    "    mixturemodel_componentmemberships = numeratorterms / repmat(\\\n",
    "                                                                         np.sum(numeratorterms,axis=1),n_components,1).T    \n",
    "    return(mixturemodel_componentmemberships)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "judicial-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_sumweights(mixturemodel_componentmemberships):    \n",
    "    # Compute total weight per component    \n",
    "    mixturemodel_weights = np.sum(mixturemodel_componentmemberships,axis=0)    \n",
    "    return(mixturemodel_weights) \n",
    "\n",
    "def run_mstep_means(X,mixturemodel_componentmemberships,mixturemodel_weights):    \n",
    "    # Update component means    \n",
    "    mixturemodel_means = scipy.sparse.lil_matrix((n_components,n_dimensions))    \n",
    "    for k in range(n_components):        \n",
    "        mixturemodel_means[k,:] = \\\n",
    "                                np.sum(scipy.sparse.diags(mixturemodel_componentmemberships[:,k]).dot(X),axis=0)        \n",
    "        mixturemodel_means[k,:] /= mixturemodel_weights[k]    \n",
    "    return(mixturemodel_means) \n",
    "\n",
    "def run_mstep_covariances(X,mixturemodel_componentmemberships,mixturemodel_weights,mixturemodel_means):    \n",
    "    # Update diagonal component covariance matrices    \n",
    "    n_dimensions = np.shape(X)[1]    \n",
    "    n_components = np.shape(mixturemodel_componentmemberships)[1]    \n",
    "    tempcovariances = np.zeros((n_components,n_dimensions))    \n",
    "    mixturemodel_covariances = []    \n",
    "    mixturemodel_inversecovariances = []    \n",
    "    for k in range(n_components):        \n",
    "        tempcovariances[k,:] = \\\n",
    "                        np.sum(scipy.sparse.diags(mixturemodel_componentmemberships[:,k]).dot(X.multiply(X)),axis=0) \\\n",
    "                                                                -mixturemodel_means[k,:].multiply(mixturemodel_means[k,:]) * mixturemodel_weights[k]        \n",
    "        tempcovariances[k,:] /= mixturemodel_weights[k]        \n",
    "        # Convert to sparse matrices        \n",
    "        tempepsilon = 1e-10        \n",
    "        # Add a small regularization term        \n",
    "        temp_covariance = scipy.sparse.diags(tempcovariances[k,:]+tempepsilon)        \n",
    "        temp_inversecovariance = scipy.sparse.diags((tempcovariances[k,:]+tempepsilon)**-1)        \n",
    "        mixturemodel_covariances.append(temp_covariance)        \n",
    "        mixturemodel_inversecovariances.append(temp_inversecovariance)     \n",
    "    return(mixturemodel_covariances,mixturemodel_inversecovariances)   \n",
    "\n",
    "def run_mstep_normalizeweights(mixturemodel_weights):    \n",
    "    # Update mixture-component prior probabilities    \n",
    "    mixturemodel_weights /= sum(mixturemodel_weights)    \n",
    "    return(mixturemodel_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "nervous-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-step 0\n",
      "E-step part2 \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "M-step 0\n",
      "M-step part1 0\n",
      "M-step part2 0\n",
      "M-step part3 0\n",
      "M-step part4 0\n"
     ]
    }
   ],
   "source": [
    "def perform_emalgorithm(X,n_components,n_emiterations):    \n",
    "    mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "                                mixturemodel_inversecovariances=initialize_mixturemodel(X,n_components)    \n",
    "    for t in range(n_emiterations):        \n",
    "        # ====== E-step: Compute the component membership        \n",
    "        # probabilities of each data point ======        \n",
    "        print('E-step ' + str(t))        \n",
    "        mixturemodel_componentmemberships=run_estep(X,mixturemodel_means,mixturemodel_covariances,\\\n",
    "                                                    mixturemodel_inversecovariances,mixturemodel_weights)            \n",
    "        # ====== M-step: update component parameters======        \n",
    "        print('M-step ' + str(t))        \n",
    "        print('M-step part1 ' + str(t))        \n",
    "        mixturemodel_weights=run_mstep_sumweights(mixturemodel_componentmemberships)        \n",
    "        print('M-step part2 ' + str(t))        \n",
    "        mixturemodel_means=run_mstep_means(X,mixturemodel_componentmemberships,mixturemodel_weights)        \n",
    "        print('M-step part3 ' + str(t))        \n",
    "        mixturemodel_covariances,mixturemodel_inversecovariances=run_mstep_covariances(X,\\\n",
    "                                                                                       mixturemodel_componentmemberships,mixturemodel_weights,mixturemodel_means)        \n",
    "        print('M-step part4 ' + str(t))        \n",
    "        mixturemodel_weights=run_mstep_normalizeweights(mixturemodel_weights)    \n",
    "        return(mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "               mixturemodel_inversecovariances) \n",
    "    \n",
    "# Try out the functions we just defined on the data \n",
    "n_components = 10 \n",
    "n_emiterations = 20\n",
    "mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    "                        mixturemodel_inversecovariances = perform_emalgorithm(X,n_components,n_emiterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "statistical-entry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "1 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "2 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "3 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "4 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "5 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "6 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "7 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "8 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n",
      "9 \\\\\n",
      "rate rat rarely rare rapidly rapid rap rank rang ran raise rainstorm rain rage rather raft quite quietly quiet \\\\\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_components):    \n",
    "    print(k, '\\\\\\\\')    \n",
    "    highest_dimensionweight_indices = \\\n",
    "                                np.argsort(-np.squeeze(\\\n",
    "                                                        mixturemodel_means[k,:].toarray()),axis=0)           \n",
    "    print(' '.join(vocabulary_pruned[\\\n",
    "                                    highest_dimensionweight_indices[1:20]]), '\\\\\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "russian-albany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[570 746 747 748 749 750 751 752 753 754]\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ”\n",
      "1\n",
      "[570 746 747 748 749 750 751 752 753 754]\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ”\n",
      "2\n",
      "[570 746 747 748 749 750 751 752 753 754]\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ”\n",
      "3\n",
      "[570 746 747 748 749 750 751 752 753 754]\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ”\n",
      "4\n",
      "[570 746 747 748 749 750 751 752 753 754]\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ”\n",
      "5\n",
      "[ 170  792   65 1071  402  362  124  797  206  892]\n",
      "“ ‘ Why , he is a man , ’ said the other , and I quite agreed with him . The farmer carried me under his arm to the cornfield , and set me up on a tall stick , where you found me . He and his friend soon after walked away and left me alone .\n",
      "6\n",
      "[ 170  792   65 1071  402  362  124  797  206  892]\n",
      "“ ‘ Why , he is a man , ’ said the other , and I quite agreed with him . The farmer carried me under his arm to the cornfield , and set me up on a tall stick , where you found me . He and his friend soon after walked away and left me alone .\n",
      "7\n",
      "[ 170  792   65 1071  402  362  124  797  206  892]\n",
      "“ ‘ Why , he is a man , ’ said the other , and I quite agreed with him . The farmer carried me under his arm to the cornfield , and set me up on a tall stick , where you found me . He and his friend soon after walked away and left me alone .\n",
      "8\n",
      "[ 170  792   65 1071  402  362  124  797  206  892]\n",
      "“ ‘ Why , he is a man , ’ said the other , and I quite agreed with him . The farmer carried me under his arm to the cornfield , and set me up on a tall stick , where you found me . He and his friend soon after walked away and left me alone .\n",
      "9\n",
      "[570 746 747 748 749 750 751 752 753 754]\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ”\n"
     ]
    }
   ],
   "source": [
    "meanterms = np.zeros((n_components)) \n",
    "logdeterminants = np.zeros((n_components)) \n",
    "logconstantterms = np.zeros((n_components)) \n",
    "\n",
    "for k in range(n_components):    \n",
    "    # Compute mu_k*inv(Sigma_k)*mu_k    \n",
    "    meanterms[k] = (mixturemodel_means[k,:] * \\\n",
    "                    mixturemodel_inversecovariances[k] * mixturemodel_means[k,:].T)[0,0] \n",
    "    # Compute terms that involve distances of data from components \n",
    "xnorms = np.zeros((n_data,n_components)) \n",
    "xtimesmu = np.zeros((n_data,n_components)) \n",
    "for k in range(n_components):    \n",
    "    xnorms[:,k] = (X * mixturemodel_inversecovariances[k] * X.T).diagonal(0)    \n",
    "    xtimesmu[:,k] = np.squeeze((X * mixturemodel_inversecovariances[k] * \\\n",
    "                                 mixturemodel_means[k,:].T).toarray()) \n",
    "xdists = xnorms + repmat(meanterms,n_data,1) - 2 * xtimesmu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "characteristic-franklin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \\\\\n",
      "[570 746 747 748 749 750 751 752 753 754] \\\\\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ” \\\\\n",
      "1 \\\\\n",
      "[570 746 747 748 749 750 751 752 753 754] \\\\\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ” \\\\\n",
      "2 \\\\\n",
      "[570 746 747 748 749 750 751 752 753 754] \\\\\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ” \\\\\n",
      "3 \\\\\n",
      "[570 746 747 748 749 750 751 752 753 754] \\\\\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ” \\\\\n",
      "4 \\\\\n",
      "[570 746 747 748 749 750 751 752 753 754] \\\\\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ” \\\\\n",
      "5 \\\\\n",
      "[ 170  792   65 1071  402  362  124  797  206  892] \\\\\n",
      "“ ‘ Why , he is a man , ’ said the other , and I quite agreed with him . The farmer carried me under his arm to the cornfield , and set me up on a tall stick , where you found me . He and his friend soon after walked away and left me alone . \\\\\n",
      "6 \\\\\n",
      "[ 170  792   65 1071  402  362  124  797  206  892] \\\\\n",
      "“ ‘ Why , he is a man , ’ said the other , and I quite agreed with him . The farmer carried me under his arm to the cornfield , and set me up on a tall stick , where you found me . He and his friend soon after walked away and left me alone . \\\\\n",
      "7 \\\\\n",
      "[ 170  792   65 1071  402  362  124  797  206  892] \\\\\n",
      "“ ‘ Why , he is a man , ’ said the other , and I quite agreed with him . The farmer carried me under his arm to the cornfield , and set me up on a tall stick , where you found me . He and his friend soon after walked away and left me alone . \\\\\n",
      "8 \\\\\n",
      "[ 170  792   65 1071  402  362  124  797  206  892] \\\\\n",
      "“ ‘ Why , he is a man , ’ said the other , and I quite agreed with him . The farmer carried me under his arm to the cornfield , and set me up on a tall stick , where you found me . He and his friend soon after walked away and left me alone . \\\\\n",
      "9 \\\\\n",
      "[570 746 747 748 749 750 751 752 753 754] \\\\\n",
      "“ I am Oz , the Great and Terrible . Who are you , and why do you seek me ? ” \\\\\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_components):    \n",
    "    tempdists = np.array(np.squeeze(xdists[:,k]))    \n",
    "    highest_componentprob_indices = np.argsort(tempdists,axis=0)    \n",
    "    print(k, '\\\\\\\\')    \n",
    "    print(highest_componentprob_indices[0:10], '\\\\\\\\')    \n",
    "    print(' '.join(wizard_oz_tokenized[highest_componentprob_indices[0]]), '\\\\\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "round-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She left Dorothy alone and went back to the others. These she also led to rooms, and each one of them found himself lodged in a very pleasant part of the Palace. Of course this politeness was wasted on the Scarecrow; for when he found himself alone in his room he stood stupidly in one spot, just within the doorway, to wait till morning. It would not rest him to lie down, and he could not close his eyes; so he remained all night staring at a little spider which was weaving its web in a corner of the room, just as if it were not one of the most wonderful rooms in the world. The Tin Woodman lay down on his bed from force of habit, for he remembered when he was made of flesh; but not being able to sleep, he passed the night moving his joints up and down to make sure they kept in good working order. The Lion would have preferred a bed of dried leaves in the forest, and did not like being shut up in a room; but he had too much sense to let this worry him, so he sprang upon the bed and rolled himself up like a cat and purred himself asleep in a minute.\n"
     ]
    }
   ],
   "source": [
    "len_longest_par = 0\n",
    "longest_par = wizard_oz_texts[0]\n",
    "number_of_longest_text = 0\n",
    "\n",
    "for idx, par in enumerate(wizard_oz_texts):\n",
    "    len_par = len(par)\n",
    "    if len_par > len_longest_par:\n",
    "        len_longest_par = len_par\n",
    "        number_of_longest_text = idx\n",
    "        longest_par = wizard_oz_texts[idx]\n",
    "\n",
    "print(longest_par)\n",
    "longest_par = wizard_oz_pruned_texts[number_of_longest_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "passing-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-301-2c1f4933b851>:37: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idfvector_max = np.log((n_docs - (np.array(dfvector.todense())[0])) / (np.array(dfvector.todense())[0]))\n"
     ]
    }
   ],
   "source": [
    "n_docs = len(wizard_oz_pruned_texts) \n",
    "n_vocab = len(vocabulary_pruned) \n",
    "# Matrix of term frequencies \n",
    "tfmatrix_len_norm = scipy.sparse.lil_matrix((n_docs,n_vocab))\n",
    "tfmatrix_log = scipy.sparse.lil_matrix((n_docs,n_vocab))\n",
    "tfmatrix_max = scipy.sparse.lil_matrix((n_docs,n_vocab))\n",
    "\n",
    "# Row vector of document frequencies \n",
    "dfvector = scipy.sparse.lil_matrix((1,n_vocab)) \n",
    "# Loop over documents \n",
    "for k in range(n_docs):    \n",
    "    # Row vector of which words occurred in this document  \n",
    "    temp_dfvector = scipy.sparse.lil_matrix((1,n_vocab))   \n",
    "    if k == number_of_longest_text:\n",
    "        temp_total_terms = len(np.unique(wizard_oz_pruned_texts[k]))\n",
    "    # Loop over words    \n",
    "    for l in range(len(wizard_oz_pruned_texts[k])):        \n",
    "    # Add current word to term-frequency count and document-count        \n",
    "        currentword = vocabulary_pruned_indices[k][l]   \n",
    "        if k == number_of_longest_text:\n",
    "            tfmatrix_len_norm[k,currentword] += 1  \n",
    "            tfmatrix_log[k,currentword] += 1\n",
    "            tfmatrix_max[k,currentword] += 1\n",
    "        temp_dfvector[0,currentword] = 1  \n",
    "    # Length normalization frequence of raw counts\n",
    "    if k == number_of_longest_text:\n",
    "        tfmatrix_len_norm /= temp_total_terms\n",
    "        max_count_word = int(np.max(np.array(tfmatrix_max.todense())))\n",
    "    # Add which words occurred in this document to overall document counts\n",
    "    dfvector = dfvector + temp_dfvector\n",
    "    # Use the count statistics to compute the tf-idf matrix \n",
    "tfmatrix_log = np.log(1 + np.array(tfmatrix_log.todense())[0])\n",
    "alpha = 0.5\n",
    "tfmatrix_max = alpha + (1 - alpha) * np.array(tfmatrix_max.todense())[0] / max_count_word\n",
    "# Let's use length-normalized frequency term count, and smoothed logarithmic idf \n",
    "idfvector = 1 + np.log((1 / (np.array(dfvector.todense())[0] + 1)) * n_docs)\n",
    "idfvector_max = np.log((n_docs - (np.array(dfvector.todense())[0])) / (np.array(dfvector.todense())[0]))\n",
    "tfidfmatrix_len_norm = tfmatrix_len_norm * idfvector\n",
    "tfidfmatrix_log = tfmatrix_log * idfvector\n",
    "tfidfmatrix_max = tfmatrix_max * idfvector_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "legitimate-replica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length-normalized frequency (TF) and Smoothed logarithmic inverse document frequency (IDF)  \\\\\n",
      "['directly' '1900' 'furiously' 'funny' 'fun' 'fully' 'full' 'fulfillment'\n",
      " 'fruit' 'front' 'frock' 'frightened' 'frighten' 'fright' 'friends'\n",
      " 'furniture' 'future' 'garden' 'garret' 'gentleman'] \\\\\n",
      "Logarithm of the count (TF) and Smoothed logarithmic inverse document frequency (IDF)  \\\\\n",
      "['1900' 'rate' 'rat' 'rarely' 'rare' 'rapidly' 'rapid' 'rap' 'rank' 'rang'\n",
      " 'ran' 'raise' 'rainstorm' 'rain' 'rage' 'rather' 'raft' 'quite' 'quietly'\n",
      " 'quiet'] \\\\\n",
      "Count relative to most frequent term (TF) and Version proportional to most common term (IDF) \\\\\n",
      "['natural' 'tumble' 'twenty' 'twin' 'cannon' 'rumble' 'kick' 'drawer'\n",
      " 'twinkling' 'ruler' 'drawing-room' 'cabinet' 'cabbage' 'twisted'\n",
      " 'polished—and' 'kitten' 'knee' 'burst' 'policeman' 'kinder'] \\\\\n"
     ]
    }
   ],
   "source": [
    "print('Length-normalized frequency (TF) and Smoothed logarithmic inverse document frequency (IDF) ', '\\\\\\\\')\n",
    "highest_tfidf_indices_len_norm = np.argsort(-1 * tfidfmatrix_len_norm,axis=0)\n",
    "top_20_tfidf_longest_par_len_norm = np.squeeze(vocabulary_pruned[highest_tfidf_indices_len_norm[:20]])\n",
    "print(top_20_tfidf_longest_par_len_norm, '\\\\\\\\')\n",
    "print('Logarithm of the count (TF) and Smoothed logarithmic inverse document frequency (IDF) ', '\\\\\\\\')\n",
    "highest_tfidf_indices_log = np.argsort(-1 * tfidfmatrix_log,axis=0)\n",
    "top_20_tfidf_longest_par_log = np.squeeze(vocabulary_pruned[highest_tfidf_indices_log[:20]])\n",
    "print(top_20_tfidf_longest_par_log, '\\\\\\\\')\n",
    "print('Count relative to most frequent term (TF) and Version proportional to most common term (IDF)', '\\\\\\\\')\n",
    "highest_tfidf_indices_max = np.argsort(-1 * tfidfmatrix_max,axis=0)\n",
    "top_20_tfidf_longest_par_max = np.squeeze(vocabulary_pruned[highest_tfidf_indices_max[:20]])\n",
    "print(top_20_tfidf_longest_par_max,'\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-range",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rising-automation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import scipy\n",
    "import sklearn \n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.manifold\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "surface-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "newsgroup_topics_documents = []\n",
    "name_docs = ['101551', '103118', '98657', '52550']\n",
    "id_docs = []\n",
    "closest_names = {}\n",
    "i = 0\n",
    "\n",
    "for topic in topics:\n",
    "    path_to_topic = 'C:/disk_D/1_tampere_uni/text data analysis/hw/6/20_newsgroups/' + topic\n",
    "    topic_documents_names = os.listdir(path_to_topic)\n",
    "    topic_documents = []\n",
    "    topic_found = True\n",
    "    for topic_document_name in topic_documents_names:\n",
    "        if str(topic_document_name) in name_docs and topic_found:\n",
    "            id_docs.append(i)\n",
    "            topic_found = False\n",
    "        document_file = open(path_to_topic + '/' + topic_document_name, 'r')\n",
    "        document = document_file.read()\n",
    "        document_file.close()\n",
    "        newsgroup_topics_documents.append(document)\n",
    "        if i in [1561, 795, 1854, 2907]:\n",
    "            closest_names[i] = topic_document_name\n",
    "            #print(f'{i} name is: {topic_document_name}')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "figured-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "excludedlinemarkers=['Xref:','Path:','From:','Newsgroups:','Subject:','Summary:', \\\n",
    "                     'Keywords:','Message-ID:','Date:','Expires:','Followup-To:','Distribution:', \\\n",
    "                     'Organization:','Approved:','Supersedes:','Lines:','NNTP-Posting-Host:', \\\n",
    "                     'References:','Sender:','In-Reply-To:','Article-I.D.:','Reply-To:', \\\n",
    "                     'Nntp-Posting-Host:'] \n",
    "\n",
    "for d_document in range(len(newsgroup_topics_documents)):\n",
    "    templines = newsgroup_topics_documents[d_document].splitlines()    \n",
    "    remaininglines = []    \n",
    "    for l in range(len(templines)):        \n",
    "        line_should_be_excluded = 0        \n",
    "        for m in range(len(excludedlinemarkers)):            \n",
    "            if len(templines[l]) >= len(excludedlinemarkers[m]):                \n",
    "                if excludedlinemarkers[m] == \\\n",
    "                                templines[l][0:len(excludedlinemarkers[m])]:                    \n",
    "                    line_should_be_excluded=1                    \n",
    "                    break        \n",
    "        if line_should_be_excluded == 0:           \n",
    "            remaininglines.append(templines[l])    \n",
    "    newsgroup_topics_documents[d_document] = '\\n'.join(remaininglines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fatty-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    temp_tokenizedtext = nltk.word_tokenize(text)    \n",
    "    mycrawled_nltktext = nltk.Text(temp_tokenizedtext)    \n",
    "    \n",
    "    return mycrawled_nltktext\n",
    "\n",
    "\n",
    "def lower_case_text(text):\n",
    "    mycrawled_lowercasetext = [] \n",
    "\n",
    "    for k in range(len(text)):        \n",
    "        lowercaseword = text[k].lower()        \n",
    "        mycrawled_lowercasetext.append(lowercaseword)    \n",
    "        \n",
    "    return mycrawled_lowercasetext\n",
    "\n",
    "def tagtowordnet(postag):   \n",
    "    wordnettag = -1   \n",
    "    if postag[0] == 'N':        \n",
    "        wordnettag = 'n'   \n",
    "    elif postag[0] == 'V':        \n",
    "        wordnettag = 'v'   \n",
    "    elif postag[0] == 'J':        \n",
    "        wordnettag = 'a'    \n",
    "    elif postag[0] == 'R':        \n",
    "        wordnettag = 'r'    \n",
    "    return(wordnettag)\n",
    "\n",
    "def lemmatizetext(nltktexttolemmatize):    \n",
    "    # Tag the text with POS tags    \n",
    "    taggedtext = nltk.pos_tag(nltktexttolemmatize)   \n",
    "    # Lemmatize each word text    \n",
    "    lemmatizedtext = []    \n",
    "    for l in range(len(taggedtext)):       \n",
    "        # Lemmatize a word using the WordNet converted POS tag       \n",
    "        wordtolemmatize = taggedtext[l][0]        \n",
    "        wordnettag = tagtowordnet(taggedtext[l][1])        \n",
    "        if wordnettag != -1:            \n",
    "            lemmatizedword = lemmatizer.lemmatize(wordtolemmatize,wordnettag)        \n",
    "        else:            \n",
    "            lemmatizedword=wordtolemmatize       \n",
    "            # Store the lemmatized word        \n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "        \n",
    "    return(lemmatizedtext) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "defensive-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_texts = []\n",
    "for document in newsgroup_topics_documents:\n",
    "    text = \" \".join(document.split())\n",
    "    newsgroup_texts.append(text)\n",
    "    \n",
    "newsgroups_tokenized = []\n",
    "for document in newsgroup_texts:\n",
    "    document = tokenize_text(document)\n",
    "    newsgroups_tokenized.append(document)\n",
    "    \n",
    "\n",
    "newsgroups_lower = []\n",
    "for document in newsgroups_tokenized:\n",
    "    document = lower_case_text(document)\n",
    "    newsgroups_lower.append(document)\n",
    "    \n",
    "\n",
    "newsgroups_texts_lemmatized = []\n",
    "for document in newsgroups_lower:\n",
    "    document = lemmatizetext(document)\n",
    "    #document = nltk.Text(document)\n",
    "    newsgroups_texts_lemmatized.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "responsible-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "# We need to create a tagged version of each document \n",
    "\n",
    "gensim_tagged_docs=[] \n",
    "\n",
    "for k in range(len(newsgroups_texts_lemmatized)):    \n",
    "    doctag='doc'+str(k)    \n",
    "    tagged_document= \\\n",
    "    gensim.models.doc2vec.TaggedDocument( \\\n",
    "                                         newsgroups_texts_lemmatized[k],[doctag])    \n",
    "    gensim_tagged_docs.append(tagged_document)\n",
    "    \n",
    "    # Create a dictionary from the documents \n",
    "gensim_dictionary = gensim.corpora.Dictionary(newsgroups_texts_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adult-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vecmodel = gensim.models.doc2vec.Doc2Vec(gensim_tagged_docs, \\\n",
    "                                             vector_size=10, window=5, min_count=1, \\\n",
    "                                             workers=4, dm_concat=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "monthly-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_find = doc2vecmodel[id_docs]\n",
    "closest_to_docs_value = []\n",
    "closest_to_docs_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "laughing-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_to_find in docs_to_find:\n",
    "    value_closest = 10e6\n",
    "    number_of_closest = 0\n",
    "    for i in range(len(newsgroups_texts_lemmatized)):\n",
    "        doc_vec = doc2vecmodel[i]\n",
    "        if (doc_vec==doc_to_find).all():\n",
    "            continue\n",
    "        dist = np.linalg.norm(doc_to_find - doc_vec)\n",
    "        if dist < value_closest:\n",
    "            value_closest = dist\n",
    "            number_of_closest = i\n",
    "    closest_to_docs_value.append(value_closest)\n",
    "    closest_to_docs_id.append(number_of_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "understood-disposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1561, 795, 1854, 2907]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_to_docs_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "generous-practice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{795: '103446', 1561: '104711', 1854: '105077', 2907: '105065'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-seattle",
   "metadata": {},
   "source": [
    "Each closest document is from another newsgroup topic.\n",
    "\n",
    "For the first document (101551, from rec.autos) the closest document is 104711 from rec.motorcycles newsgroup topic.\n",
    "These documents don't look close to each other by the sense of the content. Because 101511 is about the comparison of car prices and 104711 is just so short and looks like a conspiracy theory.\n",
    "\n",
    "For the document 103118, from rec.motorcycles the closest document is 103446 from rec.autos. \n",
    "In general these documents have different main thought but they represent comparison in some sense.\n",
    "\n",
    "For the document 98657, from rec.sport.baseball the closest document is 105077 from rec.motorcycles.\n",
    "I can't find the connection in sense between these documents.\n",
    "\n",
    "For the document 52550, from rec.sport.hockey the closest document is 105065 from rec.sport.baseball.\n",
    "In closest document (105065) I found an one mention of word \"hockey\" which correspond to the topic of the document 52550.\n",
    "\n",
    "Therefore, I have noticed that closest document to the original have less number of words. So may be it would be meaningful to make some limitations about the number of words in the found closest document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

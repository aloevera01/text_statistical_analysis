{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "decimal-travel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Vera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scipy\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "changed-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    temp_tokenizedtext = nltk.word_tokenize(text)    \n",
    "    mycrawled_nltktext = nltk.Text(temp_tokenizedtext)    \n",
    "        \n",
    "    return mycrawled_nltktext\n",
    "\n",
    "\n",
    "def lower_case_text(text):\n",
    "    mycrawled_lowercasetext = [] \n",
    "\n",
    "    for k in range(len(text)):        \n",
    "        lowercaseword = text[k].lower()        \n",
    "        mycrawled_lowercasetext.append(lowercaseword)    \n",
    "    \n",
    "    return mycrawled_lowercasetext\n",
    "\n",
    "def tagtowordnet(postag):   \n",
    "    wordnettag = -1   \n",
    "    if postag[0] == 'N':        \n",
    "        wordnettag = 'n'   \n",
    "    elif postag[0] == 'V':        \n",
    "        wordnettag = 'v'   \n",
    "    elif postag[0] == 'J':        \n",
    "        wordnettag = 'a'    \n",
    "    elif postag[0] == 'R':        \n",
    "        wordnettag = 'r'    \n",
    "    return(wordnettag)\n",
    "\n",
    "def lemmatizetext(nltktexttolemmatize):    \n",
    "    # Tag the text with POS tags    \n",
    "    taggedtext = nltk.pos_tag(nltktexttolemmatize)   \n",
    "    # Lemmatize each word text    \n",
    "    lemmatizedtext = []    \n",
    "    for l in range(len(taggedtext)):       \n",
    "        # Lemmatize a word using the WordNet converted POS tag       \n",
    "        wordtolemmatize = taggedtext[l][0]        \n",
    "        wordnettag = tagtowordnet(taggedtext[l][1])        \n",
    "        if wordnettag != -1:            \n",
    "            lemmatizedword = lemmatizer.lemmatize(wordtolemmatize,wordnettag)        \n",
    "        else:            \n",
    "            lemmatizedword=wordtolemmatize       \n",
    "            # Store the lemmatized word        \n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "        \n",
    "    return(lemmatizedtext) \n",
    "\n",
    "\n",
    "def make_vocabulary(text):\n",
    "    myvocabulary = [] \n",
    "    myindices_in_vocabulary = []\n",
    "    # Find the vocabulary of each document    \n",
    "    # Get unique words and where they occur      \n",
    "    uniqueresults = np.unique(text,return_inverse=True)   \n",
    "     # Store the vocabulary and indices\n",
    "    myvocabulary = uniqueresults[0]    \n",
    "    myindices_in_vocabulary = uniqueresults[1]    \n",
    "    \n",
    "    return myvocabulary, myindices_in_vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amended-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occur(vocab, indices):\n",
    "    occurrencecounts = np.zeros((len(vocab),1)) \n",
    "    unifiedvocabulary_meancounts = np.zeros((len(vocab),1)) \n",
    "    unifiedvocabulary_countvariances = np.zeros((len(vocab),1))\n",
    "    \n",
    "    for l in range(len(indices)):    \n",
    "        occurrencecounts[indices[l]] = \\\n",
    "                                        occurrencecounts[indices[l]] + 1\n",
    "    \n",
    "    return occurrencecounts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advised-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the html content of the \"Pride and Prejudice\"\n",
    "pride_prejudice_html = requests.get('https://www.gutenberg.org/files/1342/1342-0.txt')\n",
    "\n",
    "# getting the text from html content of the \"Pride and Prejudice\"\n",
    "pride_prejudice_text = bs4.BeautifulSoup(pride_prejudice_html.content,'html.parser')\n",
    "\n",
    "# delete /n and /r from the text of the \"Pride and Prejudice\"\n",
    "pride_prejudice_text = ' '.join(str(pride_prejudice_text).split())\n",
    "\n",
    "# tokenize a text of the \"Pride and Prejudice\"\n",
    "tokenized_pride_prejudice = tokenize_text(pride_prejudice_text)\n",
    "\n",
    "# make all words lowered case in the tokenized text of the \"Pride and Prejudice\"\n",
    "lower_cased_tokenized_pride_prejudice = lower_case_text(tokenized_pride_prejudice)\n",
    "\n",
    "# create the lemmatized text from the lowered cased and tokenized text of the \"Pride and Prejudice\"\n",
    "lemmatized_pride_prejudice = nltk.Text(lemmatizetext(lower_cased_tokenized_pride_prejudice))\n",
    "\n",
    "# create the vocabulary and its list of indices from the lemmatized text of the \"Pride and Prejudice\"\n",
    "pride_prejudice_vocabulary, indices_pride_prejudice_vocabulary = make_vocabulary(lemmatized_pride_prejudice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "editorial-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "occur_pride_prejudice = count_occur(pride_prejudice_vocabulary, indices_pride_prejudice_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "certified-caribbean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',' 'be' 'the' 'to' '.' 'of' 'and' 'have' 'her' 'i' 'a' 'in' '“' '”'\n",
      " 'she' ';' 'not' 'that' 'it' 'you' 'he' 'his' 'as' 'with' 'for' 'but' 'do'\n",
      " 'at' 'mr.' 'on' 'him' '’' 'my' 'by' 's' 'elizabeth' 'all' 'say' 'they'\n",
      " 'so' 'which' 'could' 'from' '!' 'no' 'very' 'this' 'what' 'would' '?'\n",
      " 'your' 'their' 'them' 'will' 'darcy' 'me' 'such' 'know' 'or' 'when' 'if'\n",
      " 'an' 'there' 'mrs.' 'can' 'make' 'bennet' 'think' 'much' 'more' 'must'\n",
      " 'any' 'bingley' 'see' 'jane' 'who' 'sister' 'miss' 'than' 'go' 'one'\n",
      " 'give' 'lady' 'come' 'we' 'should' 'well' 'how' 'good' 'before' 'other'\n",
      " 'herself' 'though' 'time' 'never' 'only' 'soon' 'some' 'may' 'after']\n"
     ]
    }
   ],
   "source": [
    "highest_occurrences_indices_pride_prejudice = np.argsort(\\\n",
    "                                                       -1*occur_pride_prejudice,axis=0) \n",
    "\n",
    "top_100_pride_prejudice = np.squeeze(pride_prejudice_vocabulary[\\\n",
    "                                                              highest_occurrences_indices_pride_prejudice[:100]])\n",
    "print(top_100_pride_prejudice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-patent",
   "metadata": {},
   "source": [
    "3.1 (b) lexical dispersion plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "given-liability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_1 = ['pride', 'prejudice', 'elizabeth', 'darcy', 'charlotte','exempt']\n",
    "group_2 = ['love', 'hate', 'marriage', 'husband', 'wife']\n",
    "group_3 = ['father', 'mother', 'daughter', 'family', 'dance', 'happy']\n",
    "groups = [group_1, group_2, group_3]\n",
    "\n",
    "for group in groups:\n",
    "    plot = lemmatized_pride_prejudice.dispersion_plot(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-contrary",
   "metadata": {},
   "source": [
    "Firstly, I paid attention to lexical dispersion of names of characters (Elizabeth, Darcy and Charlotte). With this information about names we can understand who is main characters (Elizabeth, Darcy) and who is the minor characters (Charlotte). In my opinion, high level of lexical dispersion of word \"love\", \"marriage\", \"husband\", \"wife\" through all book reflects the romantic (romance, melodrama) genre of the book. Also there is high level of lexical dispersion of words \"father\", \"mother\", \"daughter\", \"family\" from which it follows that events occur between characters who are connected by family ties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-webcam",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "animated-basketball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',' 'the' 'and' 'i' '.' 'of' 'be' 'to' 'my' 'a' 'in' 'have' 'that' ';'\n",
      " 'me' 'with' 'but' 'you' 'he' 'not' 'which' 'it' 'as' 'his' 'for' 'by' '“'\n",
      " 'on' 'this' 'from' 'her' 'at' 'when' '”' 'do' 'your' 'or' 'she' '!' '?'\n",
      " 'him' 'if' 'an' 'all' 'so' 'they' 'one' 'will' 'could' 'their' 'we'\n",
      " 'would' 'no' 'who' 'more' 'these' 'should' 'now' 'can' 'yet' 'before'\n",
      " 'some' '’' 'man' 'myself' 'day' 'father' 'what' 'our' 'say' 'upon' 'them'\n",
      " 'its' 'work' 'only' 'into' 'eye' 'find' 'friend' 'any' 'life' 'make'\n",
      " 'may' 'than' 'then' 'own' 'every' 'might' 'first' 'time' 'shall' 'great'\n",
      " 'take' 'feeling' 'become' 'even' 'know' 'how' 'return' 'towards']\n"
     ]
    }
   ],
   "source": [
    "# getting the html content of the \"Frankenstein\"\n",
    "frankenstein_html = requests.get('https://www.gutenberg.org/files/84/84-0.txt')\n",
    "\n",
    "# getting the text from the html content of the \"Frankenstein\"\n",
    "frankenstein_text = bs4.BeautifulSoup(frankenstein_html.content,'html.parser')\n",
    "\n",
    "# delete /n and /r from text of the \"Frankenstein\"\n",
    "frankenstein_text = ' '.join(str(frankenstein_text).split())\n",
    "\n",
    "# tokenize the text of the \"Frankenstein\"\n",
    "tokenized_frankenstein = tokenize_text(frankenstein_text)\n",
    "\n",
    "# make all words lowered case in the tokenized text of the \"Frankenstein\"\n",
    "lower_cased_tokenized_frankenstein = lower_case_text(tokenized_frankenstein)\n",
    "\n",
    "# create the lemmatized text from the lowered case and tokenized text of the \"Frankenstein\"\n",
    "lemmatized_frankenstein = nltk.Text(lemmatizetext(lower_cased_tokenized_frankenstein))\n",
    "\n",
    "# create a vocabulary and its list of indices from the lemmatized text of the \"Frankenstein\"\n",
    "frankenstein_vocabulary, indices_pride_frankenstein = make_vocabulary(lemmatized_frankenstein)\n",
    "\n",
    "# count occurance of the every word\n",
    "occur_frankenstein = count_occur(frankenstein_vocabulary, indices_pride_frankenstein)\n",
    "\n",
    "# sort indexces by the number of occurances\n",
    "highest_occurrences_indices_frankenstein = np.argsort(\\\n",
    "                                                       -1*occur_frankenstein,axis=0) \n",
    "\n",
    "# get top 100 words from the most common (top 100) indices\n",
    "top_100_frankenstein = np.squeeze(frankenstein_vocabulary[\\\n",
    "                                                              highest_occurrences_indices_frankenstein[:100]])\n",
    "print(top_100_frankenstein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conventional-patient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concordance for:  science\n",
      "Displaying 25 of 29 matches:\n",
      "cine , and those branch of physical science from which a naval adventurer might\n",
      "ch lead to my predilection for that science . when i be thirteen year of age we\n",
      "explode and that a modern system of science have be introduce which possess muc\n",
      "in the great disdain for a would-be science which could never even step within \n",
      "e branch of study appertain to that science as be build upon secure foundation \n",
      "t deeply imbue in the secret of his science . he ask me several question concer\n",
      "progress in the different branch of science appertain to natural philosophy . i\n",
      " by the modern professor of natural science . with a confusion of idea only to \n",
      "ry different when the master of the science seek immortality and power ; such v\n",
      "hose vision on which my interest in science be chiefly found . i be require to \n",
      "ry view of the present state of the science and explain many of its elementary \n",
      "get : “ the ancient teacher of this science , ” say he , “ promise impossibilit\n",
      "ent study and to devote myself to a science for which i believe myself to posse\n",
      "ave not neglect the other branch of science . a man would make but a very sorry\n",
      "r wish be to become really a man of science and not merely a petty experimental\n",
      "ould have advance far enough in the science not to derange their mechanism . he\n",
      "vate the acquaintance of the men of science of the university , and i find even\n",
      "m can conceive of the enticement of science . in other study you go as far as o\n",
      " death . i become acquaint with the science of anatomy , but this be not suffic\n",
      "rect their inquiry towards the same science , that i alone should be reserve to\n",
      "ement which every day take place in science and mechanic , i be encourage to ho\n",
      "nishing progress i have make in the science . he soon perceive that i dislike t\n",
      "ubject from my improvement , to the science itself , with a desire , as i evide\n",
      " sympathise in my taste for natural science ; and his literary pursuit differ w\n",
      " at that time i know nothing of the science of word or letter . “ the family , \n",
      "None\n",
      "\n",
      "\n",
      "Concordance for:  horror\n",
      "Displaying 25 of 49 matches:\n",
      "uld be impress with no supernatural horror . i do not ever remember to have tr\n",
      "ing-places . who shall conceive the horror of my secret toil as i dabble among\n",
      "f the dream vanish , and breathless horror and disgust fill my heart . unable \n",
      "lannel . i start from my sleep with horror ; a cold dew cover my forehead , my\n",
      " . oh ! no mortal could support the horror of that countenance . a mummy again\n",
      "extreme weakness . mingle with this horror , i felt the bitterness of disappoi\n",
      "is hand , and in a moment forgot my horror and misfortune ; i felt suddenly , \n",
      "will and power to effect purpose of horror , such as the deed which he have no\n",
      " to announce publicly ; its astound horror would be look upon as madness by th\n",
      " could make the murder memorable in horror . justine also be a girl of merit a\n",
      " place round his neck , a murmur of horror and indignation fill the court . ju\n",
      "countenance have alter . surprise , horror , and misery be strongly express . \n",
      " ignominy ? i could not sustain the horror of my situation , and when i percei\n",
      "ave before experienced sensation of horror , and i have endeavour to bestow up\n",
      " and then continue , “ i think with horror , my sweet lady , that you should b\n",
      "hetic soul , as , tear by remorse , horror , and despair , i behold those i lo\n",
      "r ’ s health be deeply shake by the horror of the recent event . elizabeth be \n",
      "myself on the grass , weigh down by horror and despair . at length i arrive at\n",
      "ve create . i tremble with rage and horror , resolve to wait his approach and \n",
      "tally be present at the trial ; his horror and indignation be uncontrollable w\n",
      " , in language which paint your own horror and render mine indelible . i sicke\n",
      " turn them from me with disdain and horror . the poor that stop at their door \n",
      "of my person be the chief object of horror with those who have formerly behold\n",
      "atha enter . who can describe their horror and consternation on behold me ? ag\n",
      "ool in have expose my person to the horror of his child . i ought to have fami\n",
      "None\n",
      "\n",
      "\n",
      "Concordance for:  monster\n",
      "Displaying 25 of 32 matches:\n",
      ", i behold the wretch—the miserable monster whom i have create . he hold up the\n",
      "walk about . i dread to behold this monster , but i fear still more that henry \n",
      "me ! save me ! ” i imagine that the monster seize me ; i struggle furiously and\n",
      "estore me to life . the form of the monster on whom i have bestow existence be \n",
      "almost begin to think that i be the monster that he say i be . he threaten exco\n",
      ", and i live in daily fear lest the monster whom i have create should perpetrat\n",
      "come home , and men appear to me as monster thirst for each other ’ s blood . y\n",
      "d of your remain friends. ” “ abhor monster ! fiend that thou art ! the torture\n",
      "y convince that i be in reality the monster that i be , i be fill with the bitt\n",
      "d of none like me . be i , then , a monster , a blot upon the earth , from whic\n",
      "accurse creator ! why do you form a monster so hideous that even _you_ turn fro\n",
      "lently . ‘ let me go , ’ he cry ; ‘ monster ! ugly wretch ! you wish to eat me \n",
      "ou must come with me. ’ “ ‘ hideous monster ! let me go . my papa be a syndic—h\n",
      "ntent me . it be true , we shall be monster , cut off from all the world ; but \n",
      "t perform my engagement and let the monster depart with his mate before i allow\n",
      "happiness . my promise fulfil , the monster would depart for ever . or ( so my \n",
      "itude . i do not doubt but that the monster follow me and would discover himsel\n",
      "will only exasperate my rage. ” the monster saw my determination in my face and\n",
      "rval—all leave behind , on whom the monster might satisfy his sanguinary and me\n",
      " at others i felt the finger of the monster already grasp my neck , and scream \n",
      "it be the watery , cloud eye of the monster , as i first saw them in my chamber\n",
      " , have fall a victim to me and the monster of my creation . i repassed , in my\n",
      "ould die to make her happy . if the monster execute his threat , death be inevi\n",
      " as if possess of magic power , the monster have blind me to his real intention\n",
      "rred . a grin be on the face of the monster ; he seem to jeer , as with his fie\n",
      "None\n",
      "\n",
      "\n",
      "Concordance for:  fear\n",
      "Displaying 25 of 66 matches:\n",
      "nd they be sufficient to conquer all fear of danger or death and to induce me \n",
      "however , lay to until the morning , fear to encounter in the dark those large\n",
      "re he be able to speak , and i often fear that his suffering have deprive him \n",
      "ong the tame scene of nature i might fear to encounter your unbelief , perhaps\n",
      "at a tale of superstition or to have fear the apparition of a spirit . darknes\n",
      "ion , listen attentively , catch and fear each sound as if it be to announce t\n",
      "if i seek to avoid the wretch whom i fear every turning of the street would pr\n",
      "y heart palpitate in the sickness of fear , and i hurry on with irregular step\n",
      "ho , on a lonely road , doth walk in fear and dread , and , have once turn rou\n",
      "dread to behold this monster , but i fear still more that henry should see him\n",
      "limb the hill or row on the lake . i fear that he will become an idle unless w\n",
      "ent to my recollection , but which i fear the detail to another would only imp\n",
      "y , might not be the less decisive . fear overcame me ; i dare no advance , dr\n",
      ", or to mock at my unhappiness ? ” i fear , my friend , that i shall render my\n",
      "et , as i draw near home , grief and fear again overcame me . night also close\n",
      "e evidence of fact a weight that , i fear , leave no hope for doubt . but she \n",
      "guiltless of this murder . i have no fear , therefore , that any circumstantia\n",
      " say i , “ and that shall be prove ; fear nothing , but let your spirit be che\n",
      "r , and they speak well of her ; but fear and hatred of the crime of which the\n",
      "ut do not mourn , dear girl . do not fear . i will proclaim , i will prove you\n",
      "ake her head mournfully . “ i do not fear to die , ” she say ; “ that pang be \n",
      "nd deprive the soul both of hope and fear . justine die , she rest , and i be \n",
      "alterable evil , and i live in daily fear lest the monster whom i have create \n",
      "the past . there be always scope for fear so long as anything i love remain be\n",
      "mighty as omnipotence—and i cease to fear or to bend before any be less almigh\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_concordance = ['science', 'horror', 'monster', 'fear']\n",
    "\n",
    "for word in group_concordance:\n",
    "    print(\"Concordance for: \", word)\n",
    "    print(lemmatized_frankenstein.concordance(word))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-asthma",
   "metadata": {},
   "source": [
    "The occurrences of the \"science\". There are different types of science met through the text: \"Physical science\", \"modern ... science\", \"natural science\", \"science of anatomy\". And collocation \"man (men) of science\" means more to scientist than to scientist(s).\n",
    "\n",
    "The occurrences of the \"horror\". \"No horror\" rather mean negation of horror. Also I have found collocation \"horror ... fill\".\n",
    "\n",
    "The occurrences of the \"monster\". \"Monster whom I have create\", \"the monster\" implies a character (Frankenstein).\n",
    "\n",
    "The occurrences of the \"fear\". I have discovered that at the beginning \"I fear\" appears many times but closer to the end there is \"no(t) fear\", \"I cease to fear\". This can convey the meaning of the book that there was fear first and then it goes away at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fatty-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header_footer(text, name_book):\n",
    "    header = \"*** START OF THIS PROJECT GUTENBERG EBOOK \" + name_book.upper() + \" ***\"\n",
    "    footer = \"*** END OF THIS PROJECT GUTENBERG EBOOK \" + name_book.upper() + \" ***\"\n",
    "    no_header_text = text[(text.find(header) + len(header)):]\n",
    "    no_footer_text = no_header_text[:no_header_text.find(footer)]\n",
    "    \n",
    "    return no_footer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "original-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_text(text, vocab, indices_vocab, high_occur_indices):\n",
    "    nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "    pruningdecisions = np.zeros((len(vocab),1)) \n",
    "    for k in range(len(vocab)):  \n",
    "        # Rule 1: check the nltk stop word list    \n",
    "        if (vocab[k] in nltkstopwords):        \n",
    "            pruningdecisions[k] = 1    \n",
    "            \n",
    "        # Rule 2: if the word is too short    \n",
    "        if len(vocab[k]) < 2:        \n",
    "            pruningdecisions[k] = 1      \n",
    "            \n",
    "        # Rule 3: if the word is too long    \n",
    "        if len(vocab[k]) > 20:        \n",
    "            pruningdecisions[k] = 1  \n",
    "            \n",
    "        # Rule 4: if the word is in the top 1% of frequent words   \n",
    "        if (k in high_occur_indices[\\\n",
    "                                    0:int(np.floor(len(vocab) * 0.01))]):        \n",
    "            pruningdecisions[k] = 1    \n",
    "        \n",
    "        # Rule 5: if the word occurs less than 4 times\n",
    "        if text.count(vocab[k]) < 4:\n",
    "            pruningdecisions[k] = 1  \n",
    "            \n",
    "    oldtopruned=[] \n",
    "    tempind=-1 \n",
    "    \n",
    "    for k in range(len(vocab)):    \n",
    "        if pruningdecisions[k] == 0:        \n",
    "            tempind += 1        \n",
    "            oldtopruned.append(tempind)    \n",
    "        else:        \n",
    "            oldtopruned.append(-1) \n",
    "            \n",
    "    #%% Create pruned texts \n",
    "    mycrawled_prunedtext = [] \n",
    "    myindices_in_prunedvocabulary = [] \n",
    "    \n",
    "    for l in range(len(text)):        \n",
    "        temp_oldindex = indices_vocab[l]                    \n",
    "        temp_newindex = oldtopruned[temp_oldindex]        \n",
    "        if temp_newindex != -1:            \n",
    "            mycrawled_prunedtext.append(vocab[temp_oldindex])    \n",
    "            myindices_in_prunedvocabulary.append(temp_newindex)\n",
    "    \n",
    "    remainingindices = np.squeeze(np.where(pruningdecisions == 0)[0]) \n",
    "    remainingvocabulary = vocab[remainingindices] \n",
    "    \n",
    "    return mycrawled_prunedtext, myindices_in_prunedvocabulary, remainingvocabulary, remainingindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "circular-isolation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one' 'thornton' 'come' 'go' 'man' 'time' 'back' 'make' 'could' 'upon'\n",
      " 'men' 'would' 'life' 'sled' 'spitz' 'know' 'françois' 'great' 'head'\n",
      " 'never' 'thing' 'foot' 'like' 'two' 'long' 'camp' 'though' 'get' 'night'\n",
      " 'take' 'way' 'run' 'eye' 'last' 'club' 'break' 'trail' 'away' 'hand'\n",
      " 'saw' 'john' 'perrault' 'till' 'call' 'three' 'hundred' 'half' 'side'\n",
      " 'hal' 'say' 'first' 'face' 'trace' 'snow' 'find' 'place' 'fire' 'wolf'\n",
      " 'every' 'wild' 'team' 'cry' 'stand' 'spring' 'leave' 'turn' 'end' 'see'\n",
      " 'teeth' 'seem' 'husky' 'old' 'body' 'many' 'work' 'another' 'sol-leks'\n",
      " 'sound' 'ice' 'water' 'behind' 'leap' 'rest' 'good' 'hold' 'become'\n",
      " 'around' 'lay' 'fell' 'mile' 'give' 'dave' 'grow' 'look' 'start' 'keep'\n",
      " 'hour' 'forest' 'love' 'travel']\n"
     ]
    }
   ],
   "source": [
    "# getting the html content of the \"The Call of the Wild\"\n",
    "call_wild_html = requests.get('https://www.gutenberg.org/files/215/215-0.txt')\n",
    "\n",
    "name_book = \"The Call of the Wild\"\n",
    "\n",
    "# getting the text from the html content of the \"The Call of the Wild\"\n",
    "call_wild_text = bs4.BeautifulSoup(call_wild_html.content,'html.parser')\n",
    "\n",
    "# delete /n and /r from text of the \"The Call of the Wild\"\n",
    "call_wild_text = ' '.join(str(call_wild_text).split())\n",
    "\n",
    "# remove extra header and footer from the text of the \"The Call of the Wild\"\n",
    "call_wild_text_removed_extra = remove_header_footer(call_wild_text, name_book)\n",
    "\n",
    "# tokenize the text of the \"The Call of the Wild\"\n",
    "tokenized_call_wild = tokenize_text(call_wild_text_removed_extra)\n",
    "\n",
    "# make all words lowered case in the tokenized text of the \"The Call of the Wild\"\n",
    "lower_cased_tokenized_call_wild = lower_case_text(tokenized_call_wild)\n",
    "\n",
    "# create the lemmatized text from the lowered case and tokenized text of the \"The Call of the Wild\"\n",
    "lemmatized_call_wild = nltk.Text(lemmatizetext(lower_cased_tokenized_call_wild))\n",
    "\n",
    "# create a vocabulary and its list of indices from the lemmatized text of the \"The Call of the Wild\"\n",
    "call_wild_vocabulary, indices_call_wild_vocabulary = make_vocabulary(lemmatized_call_wild)\n",
    "\n",
    "# count occurance of the every word\n",
    "occur_call_wild = count_occur(remain_pruned_vocabulary_call_wild, indices_pruned_vocab_call_wild)\n",
    "\n",
    "# sort indexces by the number of occurances\n",
    "highest_occurrences_indices_call_wild = np.argsort(\\\n",
    "                                                    -1*occur_call_wild,axis=0) \n",
    "\n",
    "#pruning\n",
    "pruned_call_wild, indices_pruned_vocab_call_wild, \\\n",
    "                  remain_pruned_vocabulary_call_wild, \\\n",
    "                  remain_indices_pruned_vocab_call_wild = prune_text(lemmatized_call_wild, \n",
    "                                                                call_wild_vocabulary,\n",
    "                                                                indices_call_wild_vocabulary, \n",
    "                                                                highest_occurrences_indices_call_wild)\n",
    "\n",
    "# get top 100 words from the most common (top 100) indices\n",
    "top_100_call_wild = np.squeeze(remain_pruned_vocabulary_call_wild[\\\n",
    "                                                    highest_occurrences_indices_call_wild[:100]])\n",
    "print(top_100_call_wild)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-redhead",
   "metadata": {},
   "source": [
    "# 3.3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hispanic-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dist_between_words(pruned_text, pruned_vocab, indices_pruned_vocab):\n",
    "    distanceoccurrences = scipy.sparse.lil_matrix(\\\n",
    "                                                (len(pruned_vocab),len(pruned_vocab))) \n",
    "    \n",
    "    sumdistances = scipy.sparse.lil_matrix(\\\n",
    "                                         (len(pruned_vocab),len(pruned_vocab))) \n",
    "    \n",
    "    sumabsdistances = scipy.sparse.lil_matrix(\\\n",
    "                                            (len(pruned_vocab),len(pruned_vocab))) \n",
    "    \n",
    "    sumdistancesquares = scipy.sparse.lil_matrix(\\\n",
    "                                               (len(pruned_vocab),len(pruned_vocab)))\n",
    "    \n",
    "    latestoccurrencepositions = scipy.sparse.lil_matrix(\\\n",
    "                                                        (len(pruned_vocab),len(pruned_vocab)))   \n",
    "    \n",
    "        # Loop through all word positions m of document l    \n",
    "    for m in range(len(pruned_text)):        \n",
    "            # Get the vocabulary index of the current word in position m        \n",
    "        currentword = indices_pruned_vocab[m]        \n",
    "        # Loop through previous words, counting back up to 10 words from current word       \n",
    "        windowsize = min(m,10)        \n",
    "        for n in range(windowsize):            \n",
    "            # Get the vocabulary index of the previous word in position m-n-1            \n",
    "            previousword = indices_pruned_vocab[m - n - 1]            \n",
    "            # Is this the fist time we have encountered this word while            \n",
    "            # counting back from the word at m? Then it is the closest pair. \n",
    "            if latestoccurrencepositions[currentword,previousword] < m:                            \n",
    "                # Store the occurrence of this word pair with the word at m as the 1st word                \n",
    "                distanceoccurrences[currentword,previousword] = \\\n",
    "                                distanceoccurrences[currentword,previousword] + 1\n",
    "                \n",
    "                sumdistances[currentword,previousword] = sumdistances[\\\n",
    "                                currentword,previousword] + ((m - n - 1) - m)      \n",
    "                \n",
    "                sumabsdistances[currentword,previousword] = \\\n",
    "                                sumabsdistances[currentword,previousword] + abs((m - n - 1) - m)    \n",
    "                \n",
    "                sumdistancesquares[currentword,previousword] = \\\n",
    "                                sumdistancesquares[currentword,previousword] + ((m - n - 1) - m) ** 2 \n",
    "                \n",
    "                # Store the occurrence of this word pair with the word at n as the 1st word \n",
    "                \n",
    "                distanceoccurrences[previousword,currentword] = \\\n",
    "                                distanceoccurrences[previousword,currentword] + 1                \n",
    "                sumdistances[previousword,currentword] = sumdistances[\\\n",
    "                                previousword,currentword] + (m - (m - n - 1))                \n",
    "                sumabsdistances[previousword,currentword] = \\\n",
    "                                sumabsdistances[currentword,previousword] + abs(m- (m - n - 1))                \n",
    "                sumdistancesquares[previousword,currentword] = \\\n",
    "                                sumdistancesquares[previousword,currentword] + (m - (m - n - 1)) ** 2                \n",
    "                # Mark that we found this pair while counting down from m,               \n",
    "                # so we do not count more distant occurrences of the pair                \n",
    "                latestoccurrencepositions[currentword,previousword] = m                \n",
    "                latestoccurrencepositions[previousword,currentword] = m\n",
    "\n",
    "    return distanceoccurrences, sumdistances, sumabsdistances, sumdistancesquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "adapted-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_occurrences_call_wild, sum_dist_call_wild,\\\n",
    "                sum_abs_dist_call_wild, sum_dist_squares = find_dist_between_words(pruned_call_wild, \n",
    "                                                                                   remain_pruned_vocabulary_call_wild,\n",
    "                                                                                   indices_pruned_vocab_call_wild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vertical-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_dist_between_words(pruned_vocab, dist_occurrences, sum_dist, sum_abs_dist, sum_dist_squares):\n",
    "    \n",
    "    overalldistancecount = np.sum(dist_occurrences)\n",
    "    overalldistancesum = np.sum(sum_dist)\n",
    "    overallabsdistancesum = np.sum(sum_abs_dist)\n",
    "    overalldistancesquaresum = np.sum(sum_dist_squares)\n",
    "    overalldistancemean = overalldistancesum / overalldistancecount\n",
    "    overallabsdistancemean = overallabsdistancesum / overalldistancecount\n",
    "    overalldistancevariance = overalldistancesquaresum / (overalldistancecount - 1)\\\n",
    "                                    -overalldistancecount / (overalldistancecount) * overalldistancemean \n",
    "    overallabsdistancevariance = overalldistancesquaresum / (overalldistancecount - 1)\\\n",
    "                                    -overalldistancecount / (overalldistancecount) * overallabsdistancemean\n",
    "    \n",
    "    n_vocab = len(pruned_vocab) \n",
    "    \n",
    "    distancemeans = scipy.sparse.lil_matrix((n_vocab,n_vocab)) \n",
    "    absdistancemeans = scipy.sparse.lil_matrix((n_vocab,n_vocab)) \n",
    "    distancevariances = scipy.sparse.lil_matrix((n_vocab,n_vocab)) \n",
    "    absdistancevariances = scipy.sparse.lil_matrix((n_vocab,n_vocab)) \n",
    "    \n",
    "    for m in range(n_vocab):    \n",
    "        # Find the column indices that have at least two occurrences    \n",
    "        tempindices = np.nonzero(dist_occurrences[m,:] > 1)[1]    \n",
    "        # The occurrence vector needs to be a non-sparse data type    \n",
    "        tempoccurrences = dist_occurrences[m,tempindices].todense()    \n",
    "        # Estimate mean of m-n distance    \n",
    "        distancemeans[m,tempindices] = np.squeeze(\\\n",
    "                                                   np.array(sum_dist[m,tempindices] / tempoccurrences))    \n",
    "        absdistancemeans[m,tempindices]=np.squeeze(\\\n",
    "                                                    np.array(sum_abs_dist[m,tempindices] / tempoccurrences))    \n",
    "        # Estimate variance of m-n distance    \n",
    "        \n",
    "        meanterm = distancemeans[m,tempindices].todense()    \n",
    "        meanterm = np.multiply(meanterm, meanterm)    \n",
    "        meanterm = np.multiply(tempoccurrences / (tempoccurrences - 1), meanterm)    \n",
    "        distancevariances[m,tempindices] = np.squeeze(\\\n",
    "                                                    np.array(sum_dist_squares[m,tempindices]/(tempoccurrences-1) \\\n",
    "                                                                   - meanterm))    \n",
    "        meanterm = absdistancemeans[m,tempindices].todense()    \n",
    "        meanterm = np.multiply(meanterm,meanterm)    \n",
    "        meanterm = np.multiply(tempoccurrences / (tempoccurrences - 1),meanterm)    \n",
    "        absdistancevariances[m,tempindices] = np.squeeze(\\\n",
    "                                                        np.array(sum_dist_squares[m,tempindices] / (tempoccurrences-1) \\\n",
    "                                                                      - meanterm))\n",
    "        \n",
    "    absdistancepvalues = scipy.sparse.lil_matrix((n_vocab,n_vocab)) \n",
    "        \n",
    "    for m in range(n_vocab):    \n",
    "        # Find pairs of word m    \n",
    "        tempindices = np.nonzero(dist_occurrences[m,:]>1)[1]   \n",
    "        # For computation we need to transform these to non-sparse vectors    \n",
    "        meanterm = absdistancemeans[m,tempindices].todense()    \n",
    "        varianceterm = absdistancevariances[m,tempindices].todense()    \n",
    "        occurrenceterm = dist_occurrences[m,tempindices].todense()    \n",
    "        # Compute the t-test statistic for each pair    \n",
    "        tempstatistic = (meanterm - overallabsdistancemean) / \\\n",
    "                                np.sqrt(varianceterm / occurrenceterm + \\\n",
    "                                       overallabsdistancevariance / overalldistancecount)    \n",
    "        # Compute the t-test degrees of freedom for each pair    \n",
    "        tempdf = (np.power(varianceterm / occurrenceterm + \\\n",
    "                                overallabsdistancevariance/overalldistancecount,2)) / \\\n",
    "                                        ((np.power(varianceterm / occurrenceterm,2)) / (occurrenceterm - 1) + \\\n",
    "                                             ((overallabsdistancevariance / overalldistancecount) ** 2) / \\\n",
    "                                                 (overalldistancecount - 1))\n",
    "        # Compute the t-test p-value for each pair    \n",
    "        temppvalue = scipy.stats.t.cdf(tempstatistic,tempdf)    \n",
    "        # Store the t-test p-value for each pair    \n",
    "        absdistancepvalues[m,tempindices] = np.squeeze(np.array(temppvalue))\n",
    "        \n",
    "    return distancemeans, absdistancemeans, distancevariances, absdistancevariances, absdistancepvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cosmetic-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-af29703de4c4>:57: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(varianceterm / occurrenceterm + \\\n"
     ]
    }
   ],
   "source": [
    "dist_means_call_wild, abs_dist_means_call_wild,\\\n",
    "        dist_var_call_wild, abs_dist_var_call_wild,\\\n",
    "                            abs_dist_pvalues_call_wild = statistic_dist_between_words(remain_pruned_vocabulary_call_wild,\n",
    "                                                                                      dist_occurrences_call_wild,\n",
    "                                                                                      sum_dist_call_wild,\n",
    "                                                                                      sum_abs_dist_call_wild,\n",
    "                                                                                      sum_dist_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "handed-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(num, dim):\n",
    "    \n",
    "    row = num // dim\n",
    "    col = num % dim\n",
    "    \n",
    "    return [row, col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "imperial-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_k_collocations(remain_pruned_vocab, abs_dist_means, dist_occurrences, abs_dist_pvalues, k=20, search_pairs_tags=[['NN','JJ'], ['NN','NN']], minpairoccurrences=10):\n",
    "    # length of vocabulary\n",
    "    dim = len(remain_pruned_vocab)\n",
    "    \n",
    "    # sort absolute distance means in the ascending order and get its indices\n",
    "    high_abs_mean_dist_indices_1d = np.argsort(np.array(abs_dist_means.todense()).reshape(dim ** 2))\n",
    "    \n",
    "    # sort absolute distance means in the ascending order and get its values\n",
    "    high_abs_mean_dist_1d = np.sort(np.array(abs_dist_means.todense()).reshape(dim ** 2))\n",
    "    \n",
    "    # find index of the first element with a non-zero value\n",
    "    first_non_zero = next((i for i, x in enumerate(high_abs_mean_dist_1d) if x), None)\n",
    "    \n",
    "    # save passed elements to avoid repetition (the matrix is symmetric) \n",
    "    passed_indices = []\n",
    "    \n",
    "    # save indices of found collocations (pairs of words)\n",
    "    top_words_indices = []\n",
    "    \n",
    "    # variable for iterations\n",
    "    i = first_non_zero - 1\n",
    "\n",
    "    # iterate while the number of found words is less than k\n",
    "    while len(top_words_indices) < k:\n",
    "        i += 1\n",
    "        # get indices for every word from pair\n",
    "        indices_pair = get_indices(high_abs_mean_dist_indices_1d[i], dim)\n",
    "    \n",
    "        # check a repetition\n",
    "        if indices_pair[::-1] in passed_indices:\n",
    "            continue\n",
    "            \n",
    "        # save the proccessed element\n",
    "        passed_indices.append(indices_pair)\n",
    "\n",
    "        # check a number of occurrences\n",
    "        if (dist_occurrences[indices_pair[0],indices_pair[1]] >= minpairoccurrences) and (remain_pruned_vocab[indices_pair[0]] != remain_pruned_vocab[indices_pair[1]]):\n",
    "            \n",
    "            # get words by indices\n",
    "            words_pair = [remain_pruned_vocab[indices_pair[0]], remain_pruned_vocab[indices_pair[1]]]\n",
    "\n",
    "            # get a part of speech for each word\n",
    "            words_tags = nltk.pos_tag(words_pair)\n",
    "\n",
    "            # save only part of speech of each word\n",
    "            tags = [words_tags[0][1], words_tags[1][1]]\n",
    "\n",
    "            # variable for checking the part of speech\n",
    "            is_tags = False\n",
    "\n",
    "            # check if words have neccessery part of speech\n",
    "            for search_pair_tags in search_pairs_tags:\n",
    "                if (search_pair_tags[0] in tags[0] and search_pair_tags[1] in tags[1]) or (search_pair_tags[1] in tags[0] and search_pair_tags[0] in tags[1]):\n",
    "                    is_tags = True\n",
    "\n",
    "            # print if words proccessed all checks\n",
    "            if is_tags:\n",
    "                top_words_indices.append(indices_pair)\n",
    "                print(words_pair[0], '-', words_pair[1], '. abs mean dist:', abs_dist_means[indices_pair[0], indices_pair[1]], '. pvalue: ', round(abs_dist_pvalues[indices_pair[0], indices_pair[1]], 5))\n",
    "\n",
    "        \n",
    "    return top_words_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "behavioral-premium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thornton - john . abs mean dist: 1.8409090909090908 . pvalue:  0.0\n",
      "man - hairy . abs mean dist: 3.1818181818181817 . pvalue:  nan\n",
      "red - man . abs mean dist: 3.4 . pvalue:  nan\n",
      "han - pete . abs mean dist: 3.7333333333333334 . pvalue:  nan\n",
      "sweater - man . abs mean dist: 4.0 . pvalue:  nan\n",
      "leg - fore . abs mean dist: 4.333333333333333 . pvalue:  nan\n",
      "judge - miller . abs mean dist: 4.4 . pvalue:  0.00016\n",
      "heem - dat . abs mean dist: 5.142857142857143 . pvalue:  nan\n",
      "foot - spring . abs mean dist: 5.2 . pvalue:  nan\n",
      "club - man . abs mean dist: 5.25 . pvalue:  nan\n",
      "life - time . abs mean dist: 5.357142857142857 . pvalue:  nan\n",
      "françois - call . abs mean dist: 5.363636363636363 . pvalue:  nan\n",
      "life - man . abs mean dist: 5.5 . pvalue:  2e-05\n",
      "spitz - time . abs mean dist: 5.642857142857143 . pvalue:  nan\n",
      "trail - mile . abs mean dist: 5.7 . pvalue:  nan\n",
      "camp - night . abs mean dist: 6.083333333333333 . pvalue:  nan\n",
      "spitz - shoulder . abs mean dist: 6.1 . pvalue:  nan\n",
      "hand - thornton . abs mean dist: 6.1875 . pvalue:  nan\n",
      "françois - perrault . abs mean dist: 6.321428571428571 . pvalue:  nan\n",
      "come - land . abs mean dist: 6.6 . pvalue:  nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1108, 537],\n",
       " [647, 449],\n",
       " [851, 647],\n",
       " [453, 791],\n",
       " [1080, 647],\n",
       " [590, 385],\n",
       " [540, 684],\n",
       " [479, 233],\n",
       " [383, 1023],\n",
       " [185, 647],\n",
       " [600, 1122],\n",
       " [397, 147],\n",
       " [600, 647],\n",
       " [1017, 1122],\n",
       " [1136, 683],\n",
       " [148, 734],\n",
       " [1017, 941],\n",
       " [454, 1108],\n",
       " [397, 790],\n",
       " [190, 561]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_top_k_collocations(remain_pruned_vocabulary_call_wild, abs_dist_means_call_wild, dist_occurrences_call_wild, abs_dist_pvalues_call_wild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "sharing-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findwordindex(word, vocab):    \n",
    "    for k in range(len(vocab)):        \n",
    "        if vocab[k] == word:            \n",
    "            return(k)   \n",
    "        \n",
    "    return(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "junior-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_k_collocations_with_word(word, remain_pruned_vocab, abs_dist_means, dist_occurrences, abs_dist_pvalues, k=20, search_pairs_tags=[['NN','JJ'], ['NN','NN']]):\n",
    "    word_index = findwordindex(word, remain_pruned_vocab)\n",
    "    \n",
    "    if word_index == -1:        \n",
    "        print('Word not found: ' + word)        \n",
    "        return \n",
    "    \n",
    "    lowest_meandistances_indices = np.argsort(np.squeeze(\\\n",
    "                                                          np.array(abs_dist_means[word_index,:].todense())),axis=0) \n",
    "    # save indices of found collocations (pairs of words)\n",
    "    top_words_indices = []\n",
    "    i = -1 \n",
    "    \n",
    "    while len(top_words_indices) < k and (i + 1) < len(lowest_meandistances_indices):\n",
    "        i += 1\n",
    "        index = lowest_meandistances_indices[i]\n",
    "        \n",
    "        if abs_dist_means[word_index, index] > 1:\n",
    "            words = [word, remain_pruned_vocab[index]]\n",
    "            words_tags = nltk.pos_tag(words)\n",
    "            tags = [words_tags[0][1], words_tags[1][1]]\n",
    "\n",
    "            is_tags = False\n",
    "\n",
    "            for search_pair_tags in search_pairs_tags:\n",
    "                if (search_pair_tags[0] in tags[0] and search_pair_tags[1] in tags[1]) or (search_pair_tags[1] in tags[0] and search_pair_tags[0] in tags[1]):\n",
    "                    is_tags = True\n",
    "\n",
    "            if is_tags:\n",
    "                top_words_indices.append(index)\n",
    "                print(word, '-', remain_pruned_vocab[index], '. abs mean dist:', abs_dist_means[word_index, index], '. pvalue: ', round(abs_dist_pvalues[word_index, index], 5))\n",
    "\n",
    "\n",
    "    return top_words_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "streaming-contrary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word not found: dog\n"
     ]
    }
   ],
   "source": [
    "print_top_k_collocations_with_word('dog', remain_pruned_vocabulary_call_wild, abs_dist_means_call_wild_unpruned, dist_occurrences_call_wild_unpruned, abs_dist_pvalues_call_wild_unpruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-microwave",
   "metadata": {},
   "source": [
    "I have tried to find 'dog' but got an error because 'dog' was deleted after the pruning, so we neew do the counting of occurrences and other statistics for unpruned (lemmatazied) text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "tribal-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_occurrences_call_wild_unpruned, sum_dist_call_wild_unpruned,\\\n",
    "                sum_abs_dist_call_wild_unpruned, sum_dist_squares_unpruned = find_dist_between_words(lemmatized_call_wild, \n",
    "                                                                                                     call_wild_vocabulary,\n",
    "                                                                                                     indices_call_wild_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "false-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-af29703de4c4>:57: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(varianceterm / occurrenceterm + \\\n"
     ]
    }
   ],
   "source": [
    "dist_means_call_wild_unpruned, abs_dist_means_call_wild_unpruned,\\\n",
    "        dist_var_call_wild_unpruned, abs_dist_var_call_wild_unpruned,\\\n",
    "                            abs_dist_pvalues_call_wild_unpruned = statistic_dist_between_words(call_wild_vocabulary,\n",
    "                                                                                      dist_occurrences_call_wild_unpruned,\n",
    "                                                                                      sum_dist_call_wild_unpruned,\n",
    "                                                                                      sum_abs_dist_call_wild_unpruned,\n",
    "                                                                                      sum_dist_squares_unpruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "close-cookbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog - such . abs mean dist: 1.6666666666666667 . pvalue:  0.00124\n",
      "dog - fourteen . abs mean dist: 1.8 . pvalue:  0.00061\n",
      "dog - manner . abs mean dist: 2.0 . pvalue:  0.0\n",
      "dog - secure . abs mean dist: 2.5 . pvalue:  0.02717\n",
      "dog - human . abs mean dist: 3.0 . pvalue:  nan\n",
      "dog - drive . abs mean dist: 3.0 . pvalue:  nan\n",
      "dog - southland . abs mean dist: 3.0 . pvalue:  nan\n",
      "dog - turn . abs mean dist: 3.0 . pvalue:  nan\n",
      "dog - white . abs mean dist: 3.3333333333333335 . pvalue:  0.08252\n",
      "dog - other . abs mean dist: 3.3846153846153846 . pvalue:  nan\n",
      "dog - fall . abs mean dist: 3.5 . pvalue:  0.05635\n",
      "dog - consider . abs mean dist: 3.5 . pvalue:  0.09566\n",
      "dog - pull . abs mean dist: 3.5 . pvalue:  nan\n",
      "dog - strange . abs mean dist: 3.6666666666666665 . pvalue:  nan\n",
      "dog - husky . abs mean dist: 4.0 . pvalue:  nan\n",
      "dog - buy . abs mean dist: 4.0 . pvalue:  0.07208\n",
      "dog - return . abs mean dist: 4.0 . pvalue:  0.0\n",
      "dog - fight . abs mean dist: 4.0 . pvalue:  nan\n",
      "dog - fool . abs mean dist: 4.0 . pvalue:  nan\n",
      "dog - leap . abs mean dist: 4.0 . pvalue:  nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3296,\n",
       " 1316,\n",
       " 2040,\n",
       " 2909,\n",
       " 1645,\n",
       " 979,\n",
       " 3145,\n",
       " 3598,\n",
       " 3858,\n",
       " 2337,\n",
       " 1148,\n",
       " 683,\n",
       " 2607,\n",
       " 3253,\n",
       " 1662,\n",
       " 455,\n",
       " 2777,\n",
       " 1218,\n",
       " 1279,\n",
       " 1900]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_top_k_collocations_with_word('dog', call_wild_vocabulary, abs_dist_means_call_wild_unpruned, dist_occurrences_call_wild_unpruned, abs_dist_pvalues_call_wild_unpruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-vault",
   "metadata": {},
   "source": [
    "# 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "tropical-arizona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for the first fourteen years', 'for many years', 'for many years', 'for several years', 'for nearly two years', 'for nearly six years', 'for many years', 'for five and at another for nearly two years', 'for many years', 'for many years', 'for several years']\n"
     ]
    }
   ],
   "source": [
    "# getting the html content of the \"Frankenstein\"\n",
    "frankenstein_html = requests.get('https://www.gutenberg.org/files/84/84-0.txt')\n",
    "\n",
    "# getting the text from the html content of the \"Frankenstein\"\n",
    "frankenstein_text = bs4.BeautifulSoup(frankenstein_html.content,'html.parser')\n",
    "\n",
    "# delete /n and /r from text of the \"Frankenstein\"\n",
    "frankenstein_text = ' '.join(str(frankenstein_text).split())\n",
    "\n",
    "print(re.findall('for\\s[\\w\\s]+years', frankenstein_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-blogger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
